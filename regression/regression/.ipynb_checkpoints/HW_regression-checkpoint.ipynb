{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using grader version: 1.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import grader\n",
    "%matplotlib inline  \n",
    "matplotlib.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Linear Regression\n",
    "\n",
    "In this notebook we will generate data from a linear function:  $\\textbf{y}=\\textbf{X}\\beta+\\epsilon$ and then solve for $\\hat\\beta$ using OLS (ordinary least squares) and gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 : Generate data: $\\textbf{y}=\\textbf{X}\\beta+\\epsilon$\n",
    "Here we assume $y\\approx g(X,\\beta)=\\textbf{X}\\beta+\\epsilon$ where $g$ is linear in $\\beta$ with additive noise $\\epsilon$\n",
    "\n",
    "Your function should have the following properties:\n",
    "* output `y` as an np.array with shape (`M`,`1`)\n",
    "* `generate_linear_y` should work for any arbitrary `x`, `b`, and `eps`, as long as they are the appropriate dimensions\n",
    "* do not use for-loops to calculate each `y[i]` separately, as this will be very slow for large `M` and `N`.  Instead, you should leverage [numpy linear algebra](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_y(X,b):\n",
    "    \"\"\" Write a function that generates m data points from inputs X and b\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X :   numpy.ndarray\n",
    "          x.shape must be (M,N)\n",
    "          Each row of `X` is a single data point of dimension N\n",
    "          Therefore `X` represents M data points\n",
    "        \n",
    "    b :   numpy.ndarray\n",
    "          b.shape must be (N,1)\n",
    "          Each element of `b` is a value of beta such that b=[[b1][b2]...[bN]]\n",
    "\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    y :   numpy.ndarray\n",
    "          y.shape = (M,1)\n",
    "          y[i] = X[i]b\n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENT CODE GOES HERE\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# execute this cell to grade your work\n",
    "\n",
    "from grader import test_lineary\n",
    "test_lineary(generate_linear_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2: Perform OLS Regression: $\\hat\\beta=(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}$\n",
    "Now that we can generate noisy data from a linear system, let's estimate the values of $\\beta$ from data.\n",
    "\n",
    "Your function should have the following properties:\n",
    "* output `b` as an np.array with shape (`N`,`1`)\n",
    "* `regression_ols` should work for any arbitrary `X` and `y`, as long as they are the appropriate dimensions\n",
    "* if `X` is not full column rank, you should return `None`\n",
    "* leverage numpy linear algebra (e.g., [transpose](https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html), [inverse](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html), [pseudo-inverse](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_ols(X, y):\n",
    "    \"\"\" Write a function that performs ordinary least squares\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X :   numpy.ndarray\n",
    "          X.shape must be (M,N)\n",
    "          Each row of `X` is a single data point of dimension N\n",
    "          Therefore `X` represents M data points\n",
    "        \n",
    "    y :   numpy.ndarray\n",
    "          y.shape must be (M,1)\n",
    "          Each element of `y` is an output value such that y[i] ~ X[i]b\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    b :   numpy.ndarray\n",
    "          b.shape = (N,1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENT CODE GOES HERE\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# execute this cell to grade your work\n",
    "\n",
    "from grader import test_ols\n",
    "test_ols(regression_ols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1: Polynomial Function (1-D)\n",
    "\n",
    "Write a function, `poly` that computes $f(x)=C_0+C_1\\;x+C_2\\;x^2+...C_n\\;x^n$, evaluated at some specified value for $x$.\n",
    "\n",
    "That is, if we want to compute $f(x) = 0\\cdot x^0 + 2\\cdot x^1 + 4\\cdot x^2$, evaluated at $x=2$:\n",
    "\n",
    "$$\n",
    "f(x=2) = 0\\cdot 2 + 2\\cdot 2^1 + 4\\cdot 2^2 = 20\n",
    "$$\n",
    "\n",
    "Your function `poly` should behave as follows:\n",
    "\n",
    "        >>> poly([0, 2, 4], 2)\n",
    "        20\n",
    "\n",
    "where we specify the polynomial by simply specifying its coefficients in ascending order of power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poly(coefs,x):\n",
    "    \"\"\" Returns a polynomial with coefficients `coefs`, evaluated at `x`.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        coefs : Tuple[float]\n",
    "            The polynomial coefficients in increasing order (C0, C1, C2, ...)\n",
    "            \n",
    "            This corresponds to the polynomial: C0 + C1*x + C2*(x**2) + ...\n",
    "            \n",
    "            If `coefs` is an empty tuple, then the polynomial is 0.\n",
    "            \n",
    "        x : float\n",
    "            The value at which to evaluate the derivative\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The polynomial function at x\n",
    "        \n",
    "        Examples\n",
    "        -------\n",
    "        >>> # Using the polynomial: 0 + 2*x + 4*(x ** 2)\n",
    "        >>> # compute the polynomial when x = 2\n",
    "        >>> # i.e. 2*(2) + 4*(2 ** 2)\n",
    "        >>> poly([0, 2, 4], 2)\n",
    "        20\n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENT CODE GOES HERE\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# execute this cell to grade your work\n",
    "\n",
    "from grader import test_poly\n",
    "test_poly(poly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2: Polynomial Gradient (1-D)\n",
    "\n",
    "Given that\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}x^n = nx^{n-1}\n",
    "$$\n",
    "\n",
    "Write a function, `poly_grad` that computes the derivative of a polynomial of arbitrary degree, evaluated at some specified value for $x$.\n",
    "\n",
    "That is, if we want to compute the derivative of $f(x) = 0\\;x^0 + 2\\;x^1 + 4\\;x^2$, at $x=2$:\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = 0 + 2\\;x^0 + 8\\;x^1\\\\\n",
    "\\frac{df}{dx}\\Bigr|_{x=2} = 0 + 2 + 8\\;(2)^1 = 18\n",
    "$$\n",
    "\n",
    "Accordingly, your function `poly_grad` should behave as follows:\n",
    "\n",
    "        >>> poly_grad((0, 2, 4), 2)\n",
    "        18\n",
    "\n",
    "where we specify the polynomial by simply specifying its coefficients in ascending order of power.\n",
    "\n",
    "Because we are working with a single-variable function, we can simply evaluate the derivative rather than a gradient (i.e. `poly_grad` will return a number rather than a vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poly_grad(coefs, x):\n",
    "    \"\"\" Computes the derivative of a polynomial with coefficients `coefs`, evaluated \n",
    "        at `x`.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        coefs : Tuple[float]\n",
    "            The polynomial coefficients in increasing order (C0, C1, C2, ...)\n",
    "            \n",
    "            This corresponds to the polynomial: C0 + C1*x + C2*(x**2) + ...\n",
    "            \n",
    "            If `coefs` is an empty tuple, then the polynomial is 0.\n",
    "            \n",
    "        x : float\n",
    "            The value at which to evaluate the derivative\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The derivative of the polynomial function at x\n",
    "        \n",
    "        Examples\n",
    "        -------\n",
    "        >>> # Using the polynomial: 0 + 2*x + 4*(x ** 2)\n",
    "        >>> # compute the derivative when x = 2\n",
    "        >>> # i.e. 8*(2) + 2 -> 18\n",
    "        >>> poly_grad([0, 2, 4], 2)\n",
    "        18\n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENT CODE GOES HERE\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# execute this cell to grade your work\n",
    "\n",
    "from grader import test_polygrad\n",
    "test_polygrad(poly_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3: Gradient Descent (1-D)\n",
    "Now, implement vanilla gradient descent in order *minimize* the polynomial. Refer to the course material from this module for a detailed discussion of gradient descent.\n",
    "\n",
    "This function will not only iteratively update `x` to try to minimize `f(x)`, but it will also keep track of all of the values of `x` during this process. This will allow us to make some nice visualizations of the gradient descent process.\n",
    "\n",
    "A note about your implementation: do not worry about dividing $\\frac{df}{dx}$ by $\\left|\\frac{df}{dx}\\right|$. This is a case where doing so will actually slow down our minimization process. (If you don't know what I'm talking about, revisit the section on gradient descent in the module).\n",
    "\n",
    "After you implement gradient descent and produce the visualization, see if you can envision what the gradient descent trajectory would have looked like if you had normalized the step-size, $\\delta$, by $\\left|\\frac{df}{dx}\\right|$ so that each step taken was of an equal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_descent(coefs, step_size=0.1, iterations=10, x=100.):\n",
    "    \"\"\" Returns a list of x-values visited when optimizing a polynomial through gradient descent \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        coefs : List[float]\n",
    "            Polynomial coefficients in increasing order\n",
    "            \n",
    "        step_size : Optional[float], default: 0.1\n",
    "            The magnitude of the step to take for each update of x\n",
    "            \n",
    "        iterations : Optional[int], default: 10\n",
    "            After this number of iterations, the grad_descent function should return\n",
    "            \n",
    "        x : Optional[float], default: 100.\n",
    "            The initial value of x\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        x_list : numpy.ndarray\n",
    "            A numpy array of the values of x that were visited, including the initial value of x.\n",
    "            The shape of x_list should be (iterations+1,)\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        Pseudo-code for this\n",
    "        - Initialize x_list to contain the initialized value `x`\n",
    "        - Until you've exhausted the total number of iterations\n",
    "          - Compute the gradient (just the derivative) of the polynomial with respect to x\n",
    "          - Update x using stepsize\n",
    "          - Add the new value of x to x_list\n",
    "        - Return x_list\n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENT CODE GOES HERE\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# execute this cell to grade your work\n",
    "from grader import test_grad_descent\n",
    "test_grad_descent(grad_descent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it out on a toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefs = [0,7,0,-5,1]\n",
    "npoints = 1001\n",
    "x = np.linspace(-2, 2, npoints)\n",
    "\n",
    "# Plot the function and derivative\n",
    "fig, ax = plt.subplots(figsize=(14,8))\n",
    "ax.plot(x, poly(coefs,x),label='f(x)')\n",
    "ax.plot(x,poly_grad(coefs,x),label='df(x)/dx')\n",
    "ax.plot(x,np.zeros(npoints),color = '0.5',linestyle='--')\n",
    "ax.legend()\n",
    "\n",
    "# Try a few different initializations\n",
    "for x0 in [-1.5,0.5]:\n",
    "    \n",
    "    # Perform gradient descent\n",
    "    xhat = grad_descent(coefs, step_size=0.01, iterations=20, x=x0)\n",
    "\n",
    "    # Plot\n",
    "    colors = ['green'] + ['gray'] * (len(xhat) - 2) + ['r']\n",
    "    fig, ax = plt.subplots(figsize=(14,8))\n",
    "    ax.plot(x, poly(coefs,x),label='f(x)')\n",
    "    ax.plot(xhat, poly(coefs,xhat),c='r',linestyle=':',label='$f(\\\\hat x)$')\n",
    "    ax.scatter(xhat, poly(coefs,xhat),c=colors,s=100, zorder=npoints+1)\n",
    "    ax.text(1, 30, 'Start: {:.3f}'.format(xhat[0]), color='green')\n",
    "    ax.text(1, 25, 'End:   {:.3f}'.format(xhat[-1]), color='red')\n",
    "    ax.text(1, 20, 'f($\\\\hat x$):   {:.3f}'.format(poly(coefs,xhat[-1])), color='k')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.4: Gradient Descent N-D\n",
    "Assume a linear function: \n",
    "\n",
    "$$y=f(X,\\beta) = \\beta_1x_1+\\beta_2x_2+...+\\beta_nx_n=X\\beta$$\n",
    "\n",
    "\n",
    "Given $m$ pairs of $(X,y)$, use gradient descent to solve for $\\beta$ that minimizes the squared residuals:\n",
    "\n",
    "$$S(\\beta)=(\\boldsymbol{y}-\\boldsymbol{X\\beta})^T(\\boldsymbol{y}-\\boldsymbol{X\\beta})$$\n",
    "\n",
    "Recall that the gradient of $S$ is:\n",
    "\n",
    "$$\\nabla S(\\beta)= 2\\boldsymbol{X}^T(\\boldsymbol{X\\beta}-\\boldsymbol{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradS(X,y,beta):\n",
    "    \"\"\" Returns the gradient of S(beta), where S the sum of squared residuals \n",
    "        of the linear equation: y=X*beta\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X :   numpy.ndarray\n",
    "              X.shape must be (M,N)\n",
    "              Each row of `X` is a single data point of dimension N\n",
    "              Therefore `X` represents M data points\n",
    "\n",
    "        y :   numpy.ndarray\n",
    "              y.shape must be (M,)\n",
    "              Each element of `y` is an output value such that y[i] ~ X[i]*beta\n",
    "\n",
    "        beta :  numpy.ndarray\n",
    "                beta.shape = (N,)\n",
    "\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        gS : numpy.ndarray\n",
    "             The gradient of S evaluated at beta\n",
    "             gS.shape = (N,)\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        You may find numpy.transpose() and numpy.dot() to be useful here.\n",
    "        \n",
    "        See:\n",
    "            https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html\n",
    "            https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENT CODE GOES HERE\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradS_des(X,y,beta0,step_size=0.001,iterations=20):\n",
    "    \"\"\" Returns the estimated beta value by minimizing the squared residuals S of \n",
    "        a linear equation using gradient descent \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X :   numpy.ndarray\n",
    "              X.shape must be (M,N)\n",
    "              Each row of `X` is a single data point of dimension N\n",
    "              Therefore `X` represents M data points\n",
    "\n",
    "        y :   numpy.ndarray\n",
    "              y.shape must be (M,)\n",
    "              Each element of `y` is an output value such that y[i] ~ X[i]*beta\n",
    "\n",
    "        beta0 : numpy.ndarray\n",
    "                This is the initial guess for beta\n",
    "                beta0.shape = (N,)\n",
    "                \n",
    "        step_size : Optional[float], default: 0.001\n",
    "                    The magnitude of the step to take for each update of beta\n",
    "            \n",
    "        iterations : Optional[int], default: 20\n",
    "                    After this number of iterations, the gradS_des function should return\n",
    "\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        beta :  numpy.ndarray\n",
    "                This is the final estimate of beta after #iterations\n",
    "                beta.shape = (N,)\n",
    "                \n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        Only return the final estimate of beta, not the estimates at each step like we did \n",
    "        for grad_descent() in Question 2.2 \n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENT CODE GOES HERE\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# execute this cell to grade your work\n",
    "\n",
    "from grader import test_gradS\n",
    "test_gradS(gradS_des)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it out on a toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n = 10     # number of dimensions\n",
    "m = 100     # number of data points\n",
    "\n",
    "# Generate random X values between 0--10\n",
    "X = np.random.rand(m,n)*10\n",
    "\n",
    "# Generate random beta values between 0--1\n",
    "b = np.random.rand(n,1)*5\n",
    "\n",
    "# Generate Gaussian noise\n",
    "eps = np.random.randn(m,1)\n",
    "\n",
    "# This should call the function you created in Question 1.1\n",
    "y = generate_linear_y(X,b) + eps\n",
    "\n",
    "# Perform OLS by calling the function you created in Question 1.2\n",
    "beta_ols = regression_ols(X, y)\n",
    "\n",
    "# Perform gradient descent by calling the function you created in Question 2.4\n",
    "beta_gd1 = gradS_des(X,y,beta0=b+np.random.randn(n,1),step_size=0.00001,iterations=50)\n",
    "beta_gd2 = gradS_des(X,y,beta0=b+np.random.randn(n,1),step_size=0.00001,iterations=100)\n",
    "beta_gd3 = gradS_des(X,y,beta0=b+np.random.randn(n,1),step_size=0.00001,iterations=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot $\\beta$ (true) vs. $\\hat\\beta$ (estimated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bsort=np.sort(b,axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.plot(bsort,bsort,color='0.75',linestyle='--')\n",
    "ax.scatter(b,beta_gd1,label='$\\\\hat\\\\beta$ (GD,iter=50)')\n",
    "ax.scatter(b,beta_gd2,label='$\\\\hat\\\\beta$ (GD,iter=100)')\n",
    "ax.scatter(b,beta_gd3,label='$\\\\hat\\\\beta$ (GD,iter=150)')\n",
    "ax.scatter(b,beta_ols,label='$\\\\hat\\\\beta$ (OLS)')\n",
    "ax.set_xlabel('$\\\\beta$ (true)')\n",
    "ax.set_ylabel('$\\\\hat\\\\beta$')\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
