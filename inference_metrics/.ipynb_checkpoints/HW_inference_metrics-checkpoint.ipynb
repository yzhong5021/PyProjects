{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Metrics\n",
    "In this notebook you will calculate various inference metrics for regression and classification problems, given a set of truth labels $y$ and a corresponding label estimates $\\hat{y}$.\n",
    "\n",
    "There are many statistical packages (e.g., sklearn) that can calculate these metrics for you.  These homework problems are simple enough that you should not need to use these packages (really!).  However, if you choose to, be aware that they may not always produce the same output as we are requesting.  For example, [sklearn.models.confusion_matrix()](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) does not output the confusion matrix in the same format as the grader expects.\n",
    "\n",
    "There are often multiple acceptable calculations for different metrics (e.g., incorporating normalization, weighting, etc.).  If you are unclear which version to use, refer to the Medlytics lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline  \n",
    "matplotlib.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Calculate $R^2$ of a regression model\n",
    "$R^2$ is defined in terms of the sum of squared residuals ($S$) and what is called the total sum of squares ($SS_{total}$). Where $\\bar{y}$ is the average of your $y_i$'s, the total sum of squares is:\n",
    "\n",
    "&emsp;&emsp;  $SS_{total}=\\sum_i^n{(y_i-\\bar{y})^2}$\n",
    "\n",
    "which can be thought of as a measure of the variance in the $y$'s of your data. $R^2$ is defined as:\n",
    "\n",
    "&emsp;&emsp;  $R^2=1-\\frac{S}{SS_{total}}$\n",
    "\n",
    "You can see how $R^2$ is almost like a \"scaled\" version of the sum of squared residuals. A high $S$ in relation to the total spread of the data will give an $R^2$ close to 0, whereas a lower $S$ will produce an $R^2$ closer to 1. This scaling makes it easer to evaluate whether your $R^2$ is \"good\" or not.\n",
    "\n",
    "### Examples\n",
    "Here are some examples you can try to make sure your function outputs what's expected:\n",
    "```python\n",
    ">>> y = numpy.array([[-170.], [ -57.], [-156.], [ -32.], [  -8.]])\n",
    ">>> yhat = numpy.array([[-155.], [ -64.], [-163.], [ -45.], [   5.]])\n",
    ">>> calc_R2(y,yhat)\n",
    "0.9696610854079459\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    ">>> y = numpy.array([[  42.], [ 336.], [  17.]])\n",
    ">>> yhat = numpy.array([[   3.], [ 327.], [  65.]])\n",
    ">>> calc_R2(y,yhat)\n",
    "0.9379415534206819\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_R2(y,yhat):\n",
    "    \"\"\"\n",
    "    Calculate R2 for true y and estimated yhat.  Assume the arrays are aligned, \n",
    "    i.e., the ith index of yhat corresponds to the estimate of the ith index of y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y :   numpy.ndarray\n",
    "          y.shape is (M,1)\n",
    "          Each row of `y` is a 1-dimensional float\n",
    "        \n",
    "    yhat: numpy.ndarray\n",
    "          yhat.shape is (M,1)\n",
    "        \n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    R2 :  float\n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENT CODE GOES HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell to grade your work\n",
    "from grader import test_R2\n",
    "test_R2(calc_R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Calculate the Confusion Matrix\n",
    "Given an array of truth labels $y$ and corresponding estimated labels $\\hat{y}$, calculate the confusion matrix:\n",
    "$\\begin{bmatrix}TP & FP \\\\ FN & TN\\end{bmatrix}$ where:\n",
    "\n",
    "&emsp;&emsp;  $TP$ = number of true positives\n",
    "\n",
    "&emsp;&emsp;  $FP$ = number of false positives\n",
    "\n",
    "&emsp;&emsp;  $FN$ = number of false negatives\n",
    "\n",
    "&emsp;&emsp;  $TN$ = number of true negatives\n",
    "\n",
    "### Examples\n",
    "Here are some examples you can try to make sure your function outputs what's expected:\n",
    "```python\n",
    ">>> y    = np.array([[1], [1], [1], [0], [1]])\n",
    ">>> yhat = np.array([[1], [1], [1], [0], [1]])\n",
    ">>> conf_mat(y,yhat)\n",
    "array([[4, 0],\n",
    "       [0, 1]])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    ">>> y    = np.array([[1], [1], [1], [0], [1], [0], [1], [0], [1], [1]])\n",
    ">>> yhat = np.array([[0], [1], [0], [1], [1], [0], [1], [0], [0], [0]])\n",
    ">>> conf_mat(y,yhat)\n",
    "array([[3, 1],\n",
    "       [4, 2]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conf_mat(y,yhat):\n",
    "    \"\"\"\n",
    "    Write a function that generates the confusion matrix from truth y and estimates yhat.\n",
    "    Assume that the elements in the arrays are aligned (i.e., ith entry of yhat corresponds to the ith entry of y)\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:      numpy.array \n",
    "            y.shape is (M,1)\n",
    "            y contains M data points, where each data point is a true class label: \n",
    "                0 (negative class) or 1 (positive class)\n",
    "    \n",
    "    yhat:   numpy.array \n",
    "            yhat.shape is (M,1)\n",
    "            yhat contains M data points, where each data point is an estimated class label: \n",
    "                0 (negative class) or 1 (positive class)\n",
    "           \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    cmatrix: numpy.ndarray\n",
    "             cmatrix.shape is (2,2)\n",
    "             assume cmatrix should be of the form: [[TP, FP]\n",
    "                                                    [FN, TN]]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENT CODE GOES HERE\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell to grade your work\n",
    "\n",
    "from grader import test_confusionmat\n",
    "test_confusionmat(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Calculate classification metrics from a confusion matrix\n",
    "Given the confusion matrix: $\\begin{bmatrix}TP & FP \\\\ FN & TN\\end{bmatrix}$, calculate the classifier's accuracy, F1-score, precision, recall, and specificity.\n",
    "\n",
    "### Examples\n",
    "Here are some examples you can try to make sure your function outputs what's expected:\n",
    "```python\n",
    ">>> cmatrix = np.array([[4, 0],[0, 1]])\n",
    ">>> metrics_confmat(cmatrix)\n",
    "{'F1': 1.0,\n",
    " 'accuracy': 1.0,\n",
    " 'precision': 1.0,\n",
    " 'recall': 1.0,\n",
    " 'specificity': 1.0}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    ">>> cmatrix = np.array([[5, 2],[14, 2]])\n",
    ">>> metrics_confmat(cmatrix)\n",
    "{'F1': 0.38461538461538458,\n",
    " 'accuracy': 0.30434782608695654,\n",
    " 'precision': 0.7142857142857143,\n",
    " 'recall': 0.26315789473684209,\n",
    " 'specificity': 0.5}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metrics_confmat(cmatrix):\n",
    "    \"\"\"\n",
    "    Write a function that calculates a variety of classification metrics from a confusion matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cmatrix: numpy.ndarray\n",
    "             cmatrix.shape is (2,2)\n",
    "             assume cmatrix is of the form: [[TP, FP]\n",
    "                                             [FN, TN]]\n",
    "                                             \n",
    "    Returns\n",
    "    -------\n",
    "    m:       dictionary\n",
    "             each item in dictionary should correspond to a different metric: \n",
    "                 m['accuracy']    = accuracy\n",
    "                 m['F1']          = F1-score\n",
    "                 m['precision']   = precision\n",
    "                 m['recall']      = recall (aka: true positive rate, sensitivity)\n",
    "                 m['specificity'] = specificity (aka: true negative rate)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENT CODE GOES HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell to grade your work\n",
    "from grader import test_accuracy\n",
    "test_accuracy(metrics_confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the ROC curve\n",
    "\n",
    "[scikit-learn](http://scikit-learn.org/) is another very convenient python package for performing machine learning and other data anslysis.  We will start using this package more later, but for now we will use it to visualize classification metrics when we have a classification score and sweep over the possible threshold values $T$ where :\n",
    "\n",
    "&emsp;&emsp;  $\\hat{y}_{class}=0$ if $\\hat{y}<T$ \n",
    "\n",
    "&emsp;&emsp;  $\\hat{y}_{class}=1$ if $\\hat{y}\\ge T$ \n",
    "\n",
    "If you don't already have scikit-learn installed, you can get install it from anaconda:\n",
    "\n",
    "<pre>`conda install scikit-learn`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell to plot an example ROC curve\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "y    = [   0,    0,    1,   1,   1,   0,    0,   1,   1,   1,   1,    0,    0,    0,   0,    0,   1,   1,   0,   0]\n",
    "yhat = [-2.8, -1.1, -2.7, 3.8, 0.6, 1.6, -1.1, 1.4, 4.7, 7.3, 4.9, -0.9, -8.8, -2.2, 5.2, -6.4, 1.7, 1.8, 0.1, 4.8]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(yhat,y,color='blue')\n",
    "plt.ylabel('y (true label)')\n",
    "plt.xlabel(r'$\\hat{y}$'+' (classification score)')\n",
    "plt.title('Example Classification Result')\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, yhat)\n",
    "auc = metrics.roc_auc_score(y, yhat)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr,tpr,color='blue',label='AUC: {:0.2f}'.format(auc))\n",
    "plt.plot([0,1],[0,1],color='red',linestyle='--')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell to plot an example PR curve\n",
    "\n",
    "average_precision = metrics.average_precision_score(y, yhat)\n",
    "precision, recall, _ = metrics.precision_recall_curve(y, yhat)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.step(recall, precision, color='b', where='post', label='AP={0:0.2f}'.format(average_precision))\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.legend()\n",
    "plt.title('2-class Precision-Recall Example')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
